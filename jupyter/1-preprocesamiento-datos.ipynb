{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Proyecto - Deteccion y mitigacion de ataques DDoS en redes IoT/Cloud usando ML/DL\n",
    "\n",
    "## Paso 1. Preprocesamiento de datos\n",
    "\n",
    "El preprocesamiento de datos es una etapa **esencial** en la construcción de\n",
    "modelos de ML/DL. En esta etapa, se realizan tareas como la limpieza de\n",
    "datos, la transformación de variables, la selección de características, con\n",
    "el fin de preparar los datos para el entrenamiento de los modelos propuestos.\n",
    "\n",
    "---"
   ],
   "id": "3e175b61c4ed1d4d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Importar librerias necesarias",
   "id": "65ed0ff6911b8bab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Importamos librerias necesarias\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from os.path import join as pjoin\n",
    "from os import listdir\n",
    "from os import getenv\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Cargamos las variables de entorno, para obtener la ruta de los datos. Si no\n",
    "# la encuentra, se lanza una excepción.\n",
    "load_dotenv('../.env')\n",
    "if not getenv('DATA_SOURCE_PATH'):\n",
    "    raise RuntimeError('Environment variable DATA_SOURCE_PATH not defined')\n",
    "if not getenv('SAVED_PARQUET_PATH'):\n",
    "    raise RuntimeError('Environment variable SAVED_PARQUET_PATH not defined')\n",
    "\n",
    "rnd_state: int = 36  # Se define semilla para reproducibilidad"
   ],
   "id": "a596600a74f0d96a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---",
   "id": "630537013a0d4f7e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### CIC-DDoS2019",
   "id": "28e16ea4b5f63c8b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "datasets_loaded = [] # Lista para almacenar los datasets cargados\n",
    "\n",
    "# Se define la ruta de origen de los datos en formato parquet\n",
    "data_path = pjoin(getenv('DATA_SOURCE_PATH'), 'parquet', 'cic-ddos2019')\n",
    "\n",
    "# Se cargan los datasets de la ruta especificada, en la lista datasets_loaded\n",
    "print('Loading CIC-DDoS2019 datasets...')\n",
    "for dataset_name in listdir(data_path):\n",
    "    if dataset_name.endswith('.parquet'):\n",
    "        datasets_loaded.append(pd.read_parquet(pjoin(data_path, dataset_name)))\n",
    "        print(f'    -> CIC-DDoS2019 {dataset_name.split(\"-\")[0]} - datos cargados')\n",
    "\n",
    "# Luego, se concatenan los datasets cargados en un solo DataFrame y se muestra\n",
    "# el shape (cantidad de filas y columnas) del DataFrame resultante.\n",
    "df = pd.concat(datasets_loaded, ignore_index=True)\n",
    "print(f'\\nFinal shape: {df.shape}')"
   ],
   "id": "d7b7b5b17802ca1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Previsualización de los datos\n",
    "\n",
    "A continuación, se muestra una previsualización de los datos cargados, para\n",
    "tener en cuenta las columnas y los valores que contiene."
   ],
   "id": "c24542bb6b0f8aff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.head(20)",
   "id": "c165263a16fd53ca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Valores de la columna `Label`\n",
    "\n",
    "Tal como se aprecia en la previsualización de los datos, la columna `Label`\n",
    "es la que contiene la información de la etiqueta de los datos, y por lo\n",
    "tanto, es la variable objetivo que se desea predecir. A continuación, se\n",
    "presentan los valores únicos de esta columna, y su respectiva cantidad."
   ],
   "id": "c0bc11b7b48704b1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df['Label'].value_counts()",
   "id": "731a81a8b47fe10e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Los datos presentan 18 etiquetas diferentes, las cuales plantean distintos\n",
    "protocolos de red, usualmente victimas de ataques DDoS. En este contexto, se\n",
    "plantea el uso de `LabelEncoder` para convertir la columna `Label` a valores\n",
    "numericos, y asi poder entrenar los modelos de ML/DL."
   ],
   "id": "8bbc91458a64a06e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Se aplica funcion lambda para convertir la columna 'Label' a binaria\n",
    "# df['classification'] = df['Label'].apply(lambda x: 0 if x == 'Benign' else 1)\n",
    "\n",
    "# Se usa LabelEncoder para convertir la columna 'Label' a numerica\n",
    "le = LabelEncoder()\n",
    "df['Label_encoded'] = le.fit_transform(df['Label'])\n",
    "\n",
    "# Se muestra el mapeo de las clases de la columna 'Label' a valores numericos\n",
    "for col_name, encoded_value in zip(le.classes_, le.transform(le.classes_)):\n",
    "    print(f'- {col_name:13s} -> {encoded_value}')"
   ],
   "id": "674000cbc331fda",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df.drop(columns=['Label'], inplace=True)  # Se elimina la columna 'Label'\n",
    "df.head(20)  # Se muestra una previsualización de los datos"
   ],
   "id": "2a0dfdc3d32d2ee5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df['Label_encoded'].value_counts()",
   "id": "f63b4d84d05c6cf0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Como se aprecia arriba, la columna `Label` ha sido convertida a valores\n",
    "numericos, y puede ser usada para entrenamiento como variable objetivo."
   ],
   "id": "d01df3e3b0a61402"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Caracteristicas y valores nulos\n",
    "\n",
    "En base a las columnas del DataFrame, se seleccionaran las caracteristicas\n",
    "relevantes para el entrenamiento de los modelos. Ademas, se identificaran\n",
    "aquellas columnas que contienen valores nulos, y se procedera a su\n",
    "eliminacion del DataFrame."
   ],
   "id": "70f5166fa69d71a3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Se muestran las columnas del DataFrame\n",
    "df.columns"
   ],
   "id": "93c390ffe8a721a5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Los datos presentan caracteristicas importantes relacionadas con el trafico\n",
    "de red, caracteristicas de los paquetes, entre otros. Se aprecia claramente\n",
    "que los datos vienen correctamente segmentados, y que las columnas poseen\n",
    "valores numericos que pueden ser utilizados para el entrenamiento de\n",
    "algoritmos de ML/DL. En ese contexto, la documentacion del dataset contiene\n",
    "cada columna y su respectiva descripcion, lo que facilita la seleccion de las\n",
    " caracteristicas que se adapten a los modelos propuestos."
   ],
   "id": "363aad74379d0b50"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "prev_shape = df.shape\n",
    "# Se eliminan las columnas con valores nulos\n",
    "df.dropna(axis=1, inplace=True)\n",
    "print(f'{prev_shape} - {df.shape}')"
   ],
   "id": "ae649ab9fc53a6e6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Guardar DataFrame preprocesado\n",
    "\n",
    "Una vez se obtiene la version final del DataFrame, se procede a guardarlo en\n",
    "formato `.parquet` para su posterior uso en la etapa de entrenamiento de los\n",
    "modelos"
   ],
   "id": "65fe838174c8b4b5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "save_parquet_path = pjoin(getenv('SAVED_PARQUET_PATH'), 'cic-ddos2019.parquet')\n",
    "df.to_parquet(save_parquet_path)  # Se guarda el DataFrame en formato parquet"
   ],
   "id": "8a8a5e7f624131b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---",
   "id": "748a316b1f0544eb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### N-BaIoT",
   "id": "584ce39b48a2ddcf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "datasets_loaded = [] # Lista para almacenar los datasets cargados\n",
    "nbaiot_csvs = ['benign', 'mirai', 'gafgyt']\n",
    "nbaiot_protocols = ['scan', 'tcp', 'udp', 'ack', 'combo']\n",
    "\n",
    "# Se define la ruta de origen de los datos en formato parquet\n",
    "data_path = pjoin(getenv('DATA_SOURCE_PATH'), 'nbaiot')\n",
    "\n",
    "# Se cargan los datasets de la ruta especificada, en la lista datasets_loaded\n",
    "print('Loading N-BaIoT datasets...')\n",
    "\n",
    "# Se cargan los datos benignos\n",
    "for i in range(1, 10):\n",
    "    _df = pd.read_csv(pjoin(data_path, f'{i}.benign.csv'))\n",
    "    _df['malign'] = False\n",
    "    datasets_loaded.append(_df)\n",
    "    print(f'    -> N-BaIoT {i}.benign - datos cargados')\n",
    "\n",
    "# Se recorren los datasets posibles de datos malignos\n",
    "for i in range(1, 10):\n",
    "    for botnet in nbaiot_csvs:\n",
    "        for protocol in nbaiot_protocols:\n",
    "            try:\n",
    "                _df = pd.read_csv(pjoin(data_path, f'{i}.{botnet}'\n",
    "                                                       f'.{protocol}.csv'))\n",
    "            except FileNotFoundError:\n",
    "                print(f'     X N-BaIoT {i}.{botnet}.{protocol} - no encontrado')\n",
    "            else:\n",
    "                _df['malign'] = True\n",
    "                datasets_loaded.append(_df)\n",
    "                print(f'    -> N-BaIoT {i}.{botnet}.{protocol} - datos cargados')\n",
    "\n",
    "# Luego, se concatenan los datasets cargados en un solo DataFrame y se muestra\n",
    "# el shape (cantidad de filas y columnas) del DataFrame resultante.\n",
    "df = pd.concat(datasets_loaded, ignore_index=True)\n",
    "print(f'\\nFinal shape: {df.shape}')"
   ],
   "id": "57b7bdff316deecd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Previsualización de los datos\n",
    "\n",
    "A continuación, se muestra una previsualización de los datos cargados, para\n",
    "tener en cuenta las columnas y los valores que contiene."
   ],
   "id": "df3a6fb4a08a0b45"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.head(20)  # Se muestra una previsualización de los datos",
   "id": "8139b49eeaf09668",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df['malign'].value_counts()",
   "id": "b7135e7147349258",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Caracteristicas y valores nulos\n",
    "\n",
    "En base a las columnas del DataFrame, se seleccionaran las caracteristicas\n",
    "relevantes para el entrenamiento de los modelos. Ademas, se identificaran\n",
    "aquellas columnas que contienen valores nulos, y se procedera a su\n",
    "eliminacion del DataFrame."
   ],
   "id": "979e56486191530f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Se muestran las columnas del DataFrame\n",
    "df.columns"
   ],
   "id": "3f943d4b194f970a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Los datos presentan caracteristicas importantes relacionadas con el trafico\n",
    "de red, caracteristicas de los paquetes, entre otros. Se aprecia claramente\n",
    "que los datos vienen correctamente segmentados, y que las columnas poseen\n",
    "valores numericos que pueden ser utilizados para el entrenamiento de\n",
    "algoritmos de ML/DL. En ese contexto, la documentacion del dataset contiene\n",
    "cada columna y su respectiva descripcion, lo que facilita la seleccion de las\n",
    " caracteristicas que se adapten a los modelos propuestos."
   ],
   "id": "4f0e837359cd21f6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "prev_shape = df.shape\n",
    "# Se eliminan las columnas con valores nulos\n",
    "df.dropna(axis=1, inplace=True)\n",
    "print(f'{prev_shape} - {df.shape}')"
   ],
   "id": "24064017a8abce94",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Seleccion de caracteristicas\n",
    "\n",
    "Se procedera a seleccionar las caracteristicas relevantes para el\n",
    "entrenamiento, utilizando XGBoost y Random Forest."
   ],
   "id": "806f7c3319be2cbf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Se establece uma muestra del dataframe para probar primero\n",
    "df_sample = df.sample(n=100000, random_state=rnd_state)\n",
    "\n",
    "# Se separa características y valor objetivo\n",
    "X = df_sample.drop('malign', axis=1)\n",
    "y = df_sample['malign']\n",
    "\n",
    "# Se divide el conjunto de datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=rnd_state,\n",
    "    stratify=y\n",
    ")"
   ],
   "id": "ad53b1efd81b262d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Usando XGBoost",
   "id": "2264462017a1202c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Inicializar y entrenar el modelo XGBoost\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=6,\n",
    "        random_state=rnd_state,\n",
    "        scale_pos_weight=(sum(y_train == 0) / sum(y_train == 1)),  # Manejo de clases desbalanceadas\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='logloss'\n",
    ")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Obtener la importancia de las características\n",
    "importances_xgb = xgb_model.feature_importances_\n",
    "feature_names_xgb = X_train.columns\n",
    "\n",
    "# Crear un DataFrame de importancias\n",
    "feature_importances_xgb = pd.DataFrame({\n",
    "    'feature': feature_names_xgb,\n",
    "    'importance': importances_xgb\n",
    "})\n",
    "\n",
    "# Ordenar las características por importancia\n",
    "feature_importances_xgb = feature_importances_xgb.sort_values(by='importance', ascending=False)\n",
    "\n",
    "# Visualizar las 20 características más importantes\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='importance', y='feature', data=feature_importances_xgb.head(20))\n",
    "plt.title('Top 20 Importancias de Características - XGBoost')\n",
    "plt.xlabel('Importancia')\n",
    "plt.ylabel('Características')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Seleccionar las características más importantes (por ejemplo, top 50)\n",
    "top_features_xgb = feature_importances_xgb.head(50)['feature'].tolist()\n",
    "print(f\"Top 50 características según XGBoost: {top_features_xgb}\")\n",
    "df_xgb = df_sample[top_features_xgb + ['malign']]"
   ],
   "id": "b1324a6e23bf77f9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Usando Random Forest",
   "id": "ce12b7d26ee3284c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Inicializar y entrenar el modelo Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=rnd_state, class_weight='balanced')\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Obtener la importancia de las características\n",
    "importances_rf = rf.feature_importances_\n",
    "feature_names_rf = X_train.columns\n",
    "\n",
    "# Crear un DataFrame de importancias\n",
    "feature_importances_rf = pd.DataFrame({\n",
    "    'feature': feature_names_rf,\n",
    "    'importance': importances_rf\n",
    "})\n",
    "\n",
    "# Ordenar las características por importancia\n",
    "feature_importances_rf = feature_importances_rf.sort_values(by='importance', ascending=False)\n",
    "\n",
    "# Visualizar las 20 características más importantes\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='importance', y='feature', data=feature_importances_rf.head(20))\n",
    "plt.title('Top 20 Importancias de Características - Random Forest')\n",
    "plt.xlabel('Importancia')\n",
    "plt.ylabel('Características')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Seleccionar las características más importantes (por ejemplo, top 50)\n",
    "top_features_rf = feature_importances_rf.head(50)['feature'].tolist()\n",
    "print(f\"Top 50 características según Random Forest: {top_features_rf}\")\n",
    "df_rf = df_sample[top_features_rf + ['malign']]"
   ],
   "id": "e599c73f169018ea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Guardar DataFrame preprocesado\n",
    "\n",
    "Una vez se obtiene la version final del DataFrame con la optimizacion de cada\n",
    " metodo, se procede a guardarlos en formato `.parquet` para su posterior uso\n",
    " en la etapa de entrenamiento de los modelos"
   ],
   "id": "bd1632ac1298afae"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "save_parquet_rf_path = pjoin(getenv('SAVED_PARQUET_PATH'), 'nbaiot-rf.parquet')\n",
    "save_parquet_xgb_path = pjoin(getenv('SAVED_PARQUET_PATH'), 'nbaiot-xgb.parquet')\n",
    "\n",
    "df_rf.to_parquet(save_parquet_rf_path)  # Se guarda el DataFrame en formato parquet\n",
    "df_xgb.to_parquet(save_parquet_xgb_path)  # Se guarda el DataFrame en formato parquet"
   ],
   "id": "91132a649e1f7626",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
